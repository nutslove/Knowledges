# Transformerアーキテクチャ
- 現代のほとんどのLLMはTransformerアーキテクチャを基にしている
- エンコーダとデコーダの2つの主要なコンポーネントから構成される
- エンコーダとデコーダはどちらも多層構造になっており、それらの層はSelf-Attentionメカニズムを使用している
- Self-Attentionは、入力の各部分が他の部分にどの程度関連しているかを計算するメカニズム

```
入力文: "I love cats"

[エンコーダ]
"I"    --> [ベクトル1]
"love" --> [ベクトル2]  ---> 全体の文脈を捉えたベクトル表現
"cats" --> [ベクトル3]

      ↓（エンコーダ出力）

[デコーダ]
<start>  --> "私"
"私"     --> "は"
"は"     --> "猫"
...      --> <end>
```

## エンコーダ
- 入力文をベクトル表現に変換する
- 処理の流れ:
  1. 入力文をトークン化し、各トークンをベクトルに変換
  2. Self-Attentionメカニズムを使用して、各トークンの文脈を考慮したベクトル表現を生成
  3. 最終的なベクトル表現を出力
- BERTはエンコーダのみを使用するモデル
- *ポジショナルエンコーディング* 
  - 入力トークンの順序情報を保持するために使用される
  - TransformerにはRNNのような順序処理能力がないため、入力に位置情報を追加する必要がある
## デコーダ
- エンコーダから得た文脈情報と、これまでに生成された出力トークンを使って、次のトークンを逐次生成する。
- 処理の流れ:
  1. エンコーダの出力（文脈ベクトル列）を受け取り、Decoder内部で保持
  2. Masked Self-Attentionを使って、デコーダ自身がこれまで生成した出力トークン間の文脈を考慮したベクトルを生成
     - 「デコーダ自身がこれまで生成した出力トークン」とは、例えば、翻訳タスクで "I love cats" を入力としたとき、出力は "私は 猫 が 好き" のようなもので、その中で、すでに生成された "私は", "猫" などを指す
  3. エンコーダ出力とのCross-Attentionを行い、入力文との整合性を考慮して次のトークンを予測
     - 「入力文」とは、元の入力のことで、上の例でいうと "I love cats" の部分
  4. トークンを1つずつ生成し、最終的な出力文を完成させる
- GPTはデコーダのみを使用するモデル

## Self-Attention
- Transformerアーキテクチャの中心的なメカニズム
- Self-Attentionは、入力の各部分が他の部分にどの程度関連しているか（「注意度（attention score）」）を計算するメカニズム
- 入力シーケンス全体を一度に処理し、各トークンが他のトークンにどのように注意を向けるかを学習する

### コンテキストベクトル（Context Vector）
- Self-Attentionの目標は、入力シーケンスの各要素に対して、他の要素との関係を考慮したコンテキストベクトルを各トークンごとに生成すること
- コンテキストベクトルは、シーケンス内の他のすべての要素からの情報を組み込んでおり、文章中の単語と単語の関係や関連性を理解する必要があるLLMにとって不可欠な要素
  - 例えば、**「彼は銀行に行った」という文において、「銀行」のコンテキストベクトルは、「彼」や「行った」といった他の単語との関係性を反映した情報を含む**
- **コンテキストベクトルは、埋め込みベクトルに重みを付けて計算される**
  - 「重み」とは、入力シーケンス内の各トークンが、他のトークンに対してどれくらい「注意を払うべきか」を示す数値のこと。各トークン間の関連性や重要度を定量化したものと考えることができる
- 強化された埋め込みベクトルと考えることができる

#### ■ コンテキストベクトルの計算
具体的には、Self-Attentionメカニズムでは、以下の3つのベクトルが各トークンに対して生成される。

1.  **クエリ（Query: Q）** 
    - 現在処理しているトークンが、他のトークンに対して「何を探しているか」を表すベクトル。
2.  **キー（Key: K）**
    - 他のトークンが、クエリに対して「どのような情報を提供できるか」を表すベクトル。
3.  **バリュー（Value: V）**
    - 他のトークンが実際に持つ「情報の内容」を表すベクトル。

これらのベクトルを使って、以下のステップで重みが計算される。

1.  **類似度の計算**: 現在のトークンの **クエリ（Q）** と、他のすべてのトークンの **キー（K）** との間の類似度（通常はドット積）を計算する。この類似度が高いほど、その2つのトークンは強く関連していると見なされる。
2.  **ソフトマックス関数による正規化**: 計算された類似度をソフトマックス関数に通すことで、それらの値を合計が1になるような **注意の重み（Attention Weights）** に変換する。これにより、どのトークンにどれだけの「注意」を割り当てるべきかがパーセンテージのような形で示される。

- 簡略な流れ  
  1. Attention Scoreの計算
     - 入力間のドット積としてAttentionスコアを計算
  2. Attentionの重みを計算
     - Attentionの重みはAttentionスコアを正規化したもの
  3. コンテキストベクトルの計算
     - 入力の加重和としてコンテキストベクトルを計算

#### ■ ドット積
- ドット積（内積）は、ベクトル同士の「かけ算」のようなもので、特に2つのベクトルがどれくらい同じ方向を向いているかを示す指標
  - 対応する要素をかけて全部足す
  - 結果はスカラー（単なる数）
- ドット積が大きいほど、2つのベクトルは同じ方向を向いていると解釈される
- 直交している（90度をなす）場合、値は0になる
- 逆方向を向いている場合、値は負になる
- 2つのベクトル a = (a1, a2, ..., an) と b = (b1, b2, ..., bn) のドット積は次のように計算される  
<br>

  ```math
  a・b = a1*b1 + a2*b2 + ... + an*bn
  ```  

<br>

  - コード例  
    ```python
    import torch

    # ベクトルAとBを定義（1次元のテンソル）
    a = torch.tensor([1, 2, 3])
    b = torch.tensor([4, 5, 6])

    # ドット積の計算
    dot_product = torch.dot(a, b)
    print(f"A・B のドット積は: {dot_product}")
    ```

#### ■ ソフトマックス
- ソフトマックス関数は、複数の数値(リスト)を確率分布に変換する数学関数
- Attentionメカニズムでは、生のAttentionスコアを0から1の範囲の重みに正規化し、**全ての重みの合計を1**にするために使われる

##### ▼Attentionでの実際の使用例
「私は猫が好きです」という文でのAttentionを考えてみる。「猫」という単語に注目するとき：

**生のAttentionスコア**：
- 私: 1.2
- は: 0.3
- 猫: 3.1
- が: 0.8
- 好き: 2.0

**ソフトマックス適用後の重み**：
- 私: 0.08 (8%)
- は: 0.03 (3%)
- 猫: 0.55 (55%) ← 最も高い注意
- が: 0.05 (5%)
- 好き: 0.29 (29%)

## ソフトマックスの重要な特性
1. **確率分布**：全ての値が0-1の範囲で、合計が1
2. **相対的な重要度保持**：大きな値ほど大きな重みを持つ
3. **微分可能**：機械学習での学習に適している
4. **温度パラメータ**：T（温度）で割ることで分布の鋭さを調整可能

温度の効果例（スコア [2.0, 1.0, 0.1] で）：
- T=1（通常）：[0.66, 0.24, 0.10]
- T=0.5（鋭い）：[0.84, 0.14, 0.02]
- T=2（平坦）：[0.46, 0.33, 0.21]

---

# LLM（Large Language Model）の学習
- 大規模なテキストデータセットを使用して、Attentionメカニズムを利用して事前学習され、ベースモデルが構築される
  - LLMの事前学習は入力変数と目的変数のペアを用いて、テキストにおいて次に来る単語を予測するタスクで行われる
  - 入力変数: これまでの文脈（例: "I love"）
  - 目的変数: 次に来る単語（例: "cats"）
  - モデル訓練中は、目的変数の後ろにある単語をすべてマスクし、（例: "I love [MASK]"）LLMは目的変数の後ろにある単語にアクセスできない
- 事前学習済みモデル（ベースモデル）を特定のタスクに適応させるために、ファインチューニングが行われる

## テンソル（Tensor）
- **多次元配列**のこと
- **スカラー（0次元）**、**ベクトル（1次元）**、**行列（2次元）**、およびそれ以上の次元（高次元テンソル）を持つデータ構造
- 例  
  ```python
  import torch

  # 0次元テンソル（スカラー）
  scalar = torch.tensor(3.14)
  print(f"0次元: {scalar}")  # tensor(3.1400)

  # 1次元テンソル（ベクトル）
  vector = torch.tensor([1, 2, 3, 4, 5])
  print(f"1次元: {vector}")  # tensor([1, 2, 3, 4, 5])

  # 2次元テンソル（行列）
  matrix = torch.tensor([[1, 2, 3],
                        [4, 5, 6]])
  print(f"2次元: {matrix}")
  # tensor([[1, 2, 3],
  #         [4, 5, 6]])

  # 3次元テンソル（例：RGBカラー画像 1枚）
  # shape: (channels=3, height=2, width=3)
  image = torch.tensor([[[255, 128, 64],   # Red channel
                        [32, 16, 8]],
                       [[200, 150, 100],   # Green channel
                        [50, 25, 12]],
                       [[180, 90, 45],     # Blue channel
                        [22, 11, 5]]])

  # 4次元テンソル（例：画像のバッチ）
  # shape: (batch_size=2, channels=3, height=2, width=2)
  batch_images = torch.randn(2, 3, 2, 2)
  print(f"4次元 shape: {batch_images.shape}")  # torch.Size([2, 3, 2, 2])
  ```
- テンソルは、数値データを効率的に表現し、計算するために使用される
- 深層学習フレームワーク（例: TensorFlow, PyTorch）では、テンソルが基本的なデータ構造として広く利用されている
- PyTorchのDataLoaderは、入力データセットを繰り返し処理しながら、入力変数と目的変数をテンソルとして返す

## 埋め込み層（Embedding Layer）、重み行列（Weight Matrix）
- 埋め込み層は、離散的な入力（単語、文字、カテゴリなど）を連続値のベクトルに変換する層
- 重み行列は、ニューラルネットワークの各層において、入力から出力への変換を決定する数値の集合
- 埋め込み層は、入力の各トークンを対応するベクトルに変換するための重み行列を持っている
- **埋め込み層は、トークンIDに基づいて埋め込み層の重み行列から対象の行を取り出すLookup演算を行っている**
- 例  
  ```python
  import torch
  import torch.nn as nn

  # 埋め込み層の作成（語彙サイズ5、埋め込み次元3）
  embedding_layer = nn.Embedding(5, 3)

  # 重み行列の確認
  print("重み行列（語彙サイズ × 埋め込み次元）:")
  print(embedding_layer.weight.data)
  print()
  ## 以下は出力例
  # 重み行列（語彙サイズ × 埋め込み次元）:
  # tensor([[ 0.6724,  0.4698,  0.6228],
  #        [-0.6037,  0.4422, -1.2405],
  #        [ 2.7881, -0.0756, -0.9052],
  #        [ 0.0336,  0.9864,  0.6590],
  #        [ 0.5250,  0.5011,  0.5585]])

  # トークンIDからベクトルへの変換（Lookup演算）
  token_ids = torch.tensor([0, 2, 4])
  embedded_vectors = embedding_layer(token_ids)

  print(f"入力トークンID: {token_ids}")
  print("出力埋め込みベクトル:")
  print(embedded_vectors)
  print()
  ## 以下は出力例
  # 入力トークンID: tensor([0, 2, 4])
  # 出力埋め込みベクトル:
  #  tensor([[ 0.6724,  0.4698,  0.6228],
  #          [ 2.7881, -0.0756, -0.9052],
  #          [ 0.5250,  0.5011,  0.5585]], grad_fn=<EmbeddingBackward0>)
  # ★上の重み行列のインデックス0、インデックス2、インデックス4がそれぞれトークンID 0, 2, 4に対応する埋め込みベクトルで、変換されている

  # 手動でのLookup演算（本質的な仕組み）
  print("手動Lookup（重み行列から直接取り出し）:")
  for token_id in token_ids:
      vector = embedding_layer.weight.data[token_id]  # トークンID番目の行を取得
      print(f"ID {token_id} -> {vector}")
  ```

## 単語の位置をエンコード
- Self-Attentionメカニズムには、シーケンス内のトークンの位置や順序という概念がない
- 位置を認識する埋め込みには、大きく分けて「相対位置埋め込み」と「絶対位置埋め込み」がある
- **相対位置埋め込み**: トークン間の相対的な距離を考慮する
- **絶対位置埋め込み**: 各トークンの絶対的な位置を考慮する
- 位置埋め込みも入れた埋め込みまでの流れ
  1. Rawテキストをトークン化
  2. トークンをIDに変換
  3. IDを埋め込み層に入力し、トークンIDを埋め込みベクトルに変換
  4. 3．で求めた埋め込みベクトルに、3.の埋め込み層と同じサイズの位置埋め込みを加算
  5. LLMのメイン層の入力として利用できる入力埋め込みが得られる

## Fine-tuning（ファインチューニング）
- 事前学習済み(pre-trained)のモデルを特定のタスクに適応させるための手法
  - 事前学習済みモデルは、ラベルなしの大量のデータで学習される
  - ファインチューニングは、特定のタスクに対してラベル付きのデータでモデルを再学習させる
- 少量のデータでモデルを再学習させることで、特定のドメインや用途における性能を向上させることができる
- Fine-tuning手法として「Instruction Fine-tuning」と「Classification Fine-tuning」がある
### Instruction Fine-tuning
- モデルに対して具体的な指示を与えるための手法
- 例えば、特定の質問に対する回答を生成するために、質問と回答のペアを用いてモデルを再学習させる
- e.g. テキストを翻訳するためのクエリと正しく翻訳されたテキストのペアで構成

### Classification Fine-tuning
- モデルに対してカテゴリ分類を行うための手法
- 例えば、画像を特定のカテゴリに分類するために、画像とそのカテゴリラベルを用いてモデルを再学習させる
- e.g. 画像を「犬」「猫」「鳥」などのカテゴリに分類するための画像とラベルのペアで構成

---

# ベクトル（Vector）
- 数値の配列やリストのこと
- 空間内の点や方向を表すために使用される数学的な概念
- ベクトルを通じて、複雑なデータや概念を数学的(定量的)に扱うことができる

### AIにおけるベクトルの使用例
1. **特徴ベクトル**
   - データの特徴を数値化したもの。たとえば、画像を表す際には、画像の各ピクセルの色情報がベクトルとして表される。

2. **単語ベクトル**
   - 自然言語処理において、単語や文章をベクトルとして表すことがある。この技術により、単語間の意味的な関係を計算できるようになる。
   - 例えば、文書をベクトル化する場合、各単語の出現回数などを要素とするベクトルを作成する。2つの文書が意味的に近いかどうかは、対応するベクトルの角度や距離を比較することで計算できる。

3. **状態ベクトル**
   - 機械学習のモデルが、問題を解くためのある時点での「状態」を数値で表したもの。

4. **重みベクトル**
   - 機械学習のアルゴリズムで、入力データに対してどの程度の重みを付けるかを示す数値のリスト。

---

# Token
- LLMモデルは(入力/出力)テキストをトークンという単位で分割して扱う
- 英語より日本語の方が１文字に必要なトークン数が多いといわれている
- OpenAIの場合、`tiktoken`というPythonのパッケージを使って入力/出力のトークン数を確認できる
  - https://github.com/openai/tiktoken

## Tokenizer
- Rawテキストをトークンに分割するためのツール
- トークン化の手法には、単語単位、サブワード単位、文字単位などがある
- Rawテキスト → (Tokenizerによる) トークン化 → 語彙を使って一意なIDに変換 → ベクトルに変換

### Byte pair encoding (BPE)
- トークン化の手法の一つ
- サブワード単位のトークン化を実現し、未知の単語問題を軽減する
  - 事前に定義された語彙にない単語を、より小さなサブワード単位か、場合によっては文字単位に分解する
- 頻出する文字をサブワードにマージし、頻出するサブワードを単語にマージするという方法で語彙を構築する
- GPTなどのモデルで広く使用されている
