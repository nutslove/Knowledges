# Transformerアーキテクチャ
- 現代のほとんどのLLMはTransformerアーキテクチャを基にしている

# Fine-tuning（ファインチューニング）
- 事前学習済み(pre-trained)のモデルを特定のタスクに適応させるための手法
  - 事前学習済みモデルは、ラベルなしの大量のデータで学習される
  - ファインチューニングは、特定のタスクに対してラベル付きのデータでモデルを再学習させる
- 少量のデータでモデルを再学習させることで、特定のドメインや用途における性能を向上させることができる
- Fine-tuning手法として「Instruction Fine-tuning」と「Classification Fine-tuning」がある
## Instruction Fine-tuning
- モデルに対して具体的な指示を与えるための手法
- 例えば、特定の質問に対する回答を生成するために、質問と回答のペアを用いてモデルを再学習させる
- e.g. テキストを翻訳するためのクエリと正しく翻訳されたテキストのペアで構成

## Classification Fine-tuning
- モデルに対してカテゴリ分類を行うための手法
- 例えば、画像を特定のカテゴリに分類するために、画像とそのカテゴリラベルを用いてモデルを再学習させる
- e.g. 画像を「犬」「猫」「鳥」などのカテゴリに分類するための画像とラベルのペアで構成

---

# その他
## ベクトル（Vector）
- 数値の配列やリストのこと
- 空間内の点や方向を表すために使用される数学的な概念
- ベクトルを通じて、複雑なデータや概念を数学的(定量的)に扱うことができる

### AIにおけるベクトルの使用例
1. **特徴ベクトル**
   - データの特徴を数値化したもの。たとえば、画像を表す際には、画像の各ピクセルの色情報がベクトルとして表される。

2. **単語ベクトル**
   - 自然言語処理において、単語や文章をベクトルとして表すことがある。この技術により、単語間の意味的な関係を計算できるようになる。
   - 例えば、文書をベクトル化する場合、各単語の出現回数などを要素とするベクトルを作成する。2つの文書が意味的に近いかどうかは、対応するベクトルの角度や距離を比較することで計算できる。

3. **状態ベクトル**
   - 機械学習のモデルが、問題を解くためのある時点での「状態」を数値で表したもの。

4. **重みベクトル**
   - 機械学習のアルゴリズムで、入力データに対してどの程度の重みを付けるかを示す数値のリスト。

## Token
- LLMモデルは(入力/出力)テキストをトークンという単位で分割して扱う
- 英語より日本語の方が１文字に必要なトークン数が多いといわれている
- OpenAIの場合、`tiktoken`というPythonのパッケージを使って入力/出力のトークン数を確認できる
  - https://github.com/openai/tiktoken
