# Transformerアーキテクチャ
- 現代のほとんどのLLMはTransformerアーキテクチャを基にしている
- エンコーダとデコーダの2つの主要なコンポーネントから構成される
- エンコーダとデコーダはどちらも多層構造になっており、それらの層はSelf-Attentionメカニズムを使用している
- Self-Attentionは、入力の各部分が他の部分にどの程度関連しているかを計算するメカニズム

```
入力文: "I love cats"

[エンコーダ]
"I"    --> [ベクトル1]
"love" --> [ベクトル2]  ---> 全体の文脈を捉えたベクトル表現
"cats" --> [ベクトル3]

      ↓（エンコーダ出力）

[デコーダ]
<start>  --> "私"
"私"     --> "は"
"は"     --> "猫"
...      --> <end>
```

## エンコーダ
- 入力文をベクトル表現に変換する
- 処理の流れ:
  1. 入力文をトークン化し、各トークンをベクトルに変換
  2. Self-Attentionメカニズムを使用して、各トークンの文脈を考慮したベクトル表現を生成
  3. 最終的なベクトル表現を出力
- BERTはエンコーダのみを使用するモデル
- *ポジショナルエンコーディング* 
  - 入力トークンの順序情報を保持するために使用される
  - TransformerにはRNNのような順序処理能力がないため、入力に位置情報を追加する必要がある
## デコーダ
- エンコーダから得た文脈情報と、これまでに生成された出力トークンを使って、次のトークンを逐次生成する。
- 処理の流れ:
  1. エンコーダの出力（文脈ベクトル列）を受け取り、Decoder内部で保持
  2. Masked Self-Attentionを使って、デコーダ自身がこれまで生成した出力トークン間の文脈を考慮したベクトルを生成
     - 「デコーダ自身がこれまで生成した出力トークン」とは、例えば、翻訳タスクで "I love cats" を入力としたとき、出力は "私は 猫 が 好き" のようなもので、その中で、すでに生成された "私は", "猫" などを指す
  3. エンコーダ出力とのCross-Attentionを行い、入力文との整合性を考慮して次のトークンを予測
     - 「入力文」とは、元の入力のことで、上の例でいうと "I love cats" の部分
  4. トークンを1つずつ生成し、最終的な出力文を完成させる
- GPTはデコーダのみを使用するモデル

# LLM（Large Language Model）の学習
- 大規模なテキストデータセットを使用して、Attentionメカニズムを利用して事前学習され、ベースモデルが構築される
  - LLMの事前学習は入力変数と目的変数のペアを用いて、テキストにおいて次に来る単語を予測するタスクで行われる
  - 入力変数: これまでの文脈（例: "I love"）
  - 目的変数: 次に来る単語（例: "cats"）
  - モデル訓練中は、目的変数の後ろにある単語をすべてマスクし、（例: "I love [MASK]"）LLMは目的変数の後ろにある単語にアクセスできない
- 事前学習済みモデル（ベースモデル）を特定のタスクに適応させるために、ファインチューニングが行われる

## Fine-tuning（ファインチューニング）
- 事前学習済み(pre-trained)のモデルを特定のタスクに適応させるための手法
  - 事前学習済みモデルは、ラベルなしの大量のデータで学習される
  - ファインチューニングは、特定のタスクに対してラベル付きのデータでモデルを再学習させる
- 少量のデータでモデルを再学習させることで、特定のドメインや用途における性能を向上させることができる
- Fine-tuning手法として「Instruction Fine-tuning」と「Classification Fine-tuning」がある
### Instruction Fine-tuning
- モデルに対して具体的な指示を与えるための手法
- 例えば、特定の質問に対する回答を生成するために、質問と回答のペアを用いてモデルを再学習させる
- e.g. テキストを翻訳するためのクエリと正しく翻訳されたテキストのペアで構成

### Classification Fine-tuning
- モデルに対してカテゴリ分類を行うための手法
- 例えば、画像を特定のカテゴリに分類するために、画像とそのカテゴリラベルを用いてモデルを再学習させる
- e.g. 画像を「犬」「猫」「鳥」などのカテゴリに分類するための画像とラベルのペアで構成

---

# ベクトル（Vector）
- 数値の配列やリストのこと
- 空間内の点や方向を表すために使用される数学的な概念
- ベクトルを通じて、複雑なデータや概念を数学的(定量的)に扱うことができる

### AIにおけるベクトルの使用例
1. **特徴ベクトル**
   - データの特徴を数値化したもの。たとえば、画像を表す際には、画像の各ピクセルの色情報がベクトルとして表される。

2. **単語ベクトル**
   - 自然言語処理において、単語や文章をベクトルとして表すことがある。この技術により、単語間の意味的な関係を計算できるようになる。
   - 例えば、文書をベクトル化する場合、各単語の出現回数などを要素とするベクトルを作成する。2つの文書が意味的に近いかどうかは、対応するベクトルの角度や距離を比較することで計算できる。

3. **状態ベクトル**
   - 機械学習のモデルが、問題を解くためのある時点での「状態」を数値で表したもの。

4. **重みベクトル**
   - 機械学習のアルゴリズムで、入力データに対してどの程度の重みを付けるかを示す数値のリスト。

# Token
- LLMモデルは(入力/出力)テキストをトークンという単位で分割して扱う
- 英語より日本語の方が１文字に必要なトークン数が多いといわれている
- OpenAIの場合、`tiktoken`というPythonのパッケージを使って入力/出力のトークン数を確認できる
  - https://github.com/openai/tiktoken

## Tokenizer
- Rawテキストをトークンに分割するためのツール
- トークン化の手法には、単語単位、サブワード単位、文字単位などがある
- Rawテキスト → (Tokenizerによる) トークン化 → 語彙を使って一意なIDに変換 → ベクトルに変換

### Byte pair encoding (BPE)
- トークン化の手法の一つ
- サブワード単位のトークン化を実現し、未知の単語問題を軽減する
  - 事前に定義された語彙にない単語を、より小さなサブワード単位か、場合によっては文字単位に分解する
- 頻出する文字をサブワードにマージし、頻出するサブワードを単語にマージするという方法で語彙を構築する
- GPTなどのモデルで広く使用されている